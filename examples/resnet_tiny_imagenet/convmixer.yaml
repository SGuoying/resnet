run_name: convmixer # Name of the  training run used for checkpointing and other logging
is_train: true             # Trains the model if true, otherwise runs evaluation
seed: 17                   # Random seed
max_duration: 90ep         # Duration to train specified as a Time string
grad_accum: 2           # Amount of gradient accumulation, 'auto' means Composer will choose the optimal value

# Model
model:
  name: convmixer           # Name of the ResNet model to train either resnet{18, 34, 50, 101, 152}
  hidden_dim: 256
  kernel_size: 5
  patch_size: 2
  num_layers: 12
  loss_name: # cross_entropy # Name of the loss function either 'cross_entropy' or 'binary_cross_entropy'
  num_classes: 1000        # Number of classes in the classification task

# Training Dataset Parameters
train_dataset:
  is_streaming: false                    # Whether or not your data is in a remote location (e.g. a S3 bucket)
  path: /home/feng/data      # Path to S3 bucket if streaming, otherwise path to local data directory
  local:  # Local cache when streaming data
  resize_size:                        # Training image resize size before crop, -1 means no resize
  crop_size:                        # Training image crop size
  batch_size: 128                      # Training dataloader batch size per device
  num_workers: 2

# Validation Dataset Parameters
eval_dataset:
  is_streaming: false                    # Whether or not your data is in a remote location (e.g. a S3 bucket)
  path: /home/feng/data      # S3 bucket if streaming, otherwise path to local data
  local:  # Local cache when streaming data
  resize_size:                     # Evaluation image resize size before crop
  crop_size:                      # Evaluation image crop size
  batch_size: 128                      # Evaluation dataloader batch size per device
  num_workers: 2

# Optimizer Parameters
optimizer:
  lr: 2.048
  momentum: 0.875
  weight_decay: 5.0e-4

# LR Scheduler Parameters
scheduler:
  t_warmup: 8ep # Duration of learning rate warmup specified as a Time string
  alpha_f: 0.0  # Base learning rate multiplier to decay to

loggers:
  progress_bar: {}
  wandb:     # Uncomment and fill below arguments to use WandB logger
    entity: keepsimpler # Name of WandB entity, usually username or organization name
    project: convmixer_tiny_imagenet # Name of WandB project
    group:   # Name of WandB group

# null for baseline or for recipe, either ["mild", "medium", "hot"] in order of increasing training time and accuracy
recipe_name: mild

# Updated parameters for mild recipe
mild:
  model.loss_name: 
  train_dataset.crop_size: 
  eval_dataset.resize_size: 
  max_duration: 36ep

# Updated parameters for medium recipe
medium:
  model.loss_name: binary_cross_entropy
  train_dataset.crop_size: 176
  eval_dataset.resize_size: 232
  max_duration: 135ep

# Updated parameters for hot recipe
hot:
  model.loss_name: binary_cross_entropy
  train_dataset.crop_size: 176
  eval_dataset.resize_size: 232
  max_duration: 270ep

# Save checkpoint parameters
save_folder:   convmixer                 # e.g. './{run_name}/ckpt' (local) or 's3://mybucket/mydir/{run_name}/ckpt' (remote)
save_interval: 1ep             # Interval to checkpoint based on time string
save_num_checkpoints_to_keep: 1 # Cleans up checkpoints saved locally only!

# Load checkpoint parameters
load_path:      # e.g. './ckpt/latest-rank{rank}.pt' (local) or 's3://mybucket/mydir/ckpt/latest-rank{rank}.pt' (remote)
